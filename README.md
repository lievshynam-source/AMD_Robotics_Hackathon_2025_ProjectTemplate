**### Make a fork or copy of this repo and fill in your team submission details! ###**

# AMD Robotics Hackathon 2025: A Human-Robot Co-Creative Interaction

## Team Information

**Team:** 21
Team name T-800 
- Maryna Lievshyna
- Afonso Diela
- Oliver Fuchs
- Joshua Tarabusi 

**Summary:** *An interactive human–robot creative game for children who struggle with social interaction (autism, social anxiety, introversion). Instead of competitive gameplay or screens, the robot becomes a calm, predictable play partner that supports turn-taking and shared attention through drawing. The human and robot roll a physical dice and draw shapes on a shared space, creating a simple, safe, and engaging interaction. This approach can be applied in therapeutic settings, hospitals, schools, or at home as a low-cost, accessible support for social development.*

*< [Images or video demonstrating your project](https://drive.google.com/file/d/1CRgT-3jYfOybKNrcDjm-S0Md_wfhv_Qg/view?usp=sharing) >*

## Submission Details

### 1. Mission Description
*Our mission explores how robots can support social engagement for children who struggle with peer interaction, including children with autism, social anxiety, or introverted traits. When social environments feel overwhelming, a robot can offer a calm, predictable, and non-judgmental presence.

Through simple co-creative interactions, we investigate how human–robot play can create safe moments of connection, helping children engage, express themselves, and gradually build confidence in shared activities.*

### 2. Creativity
- *What is novel or unique in your approach?*
- A co-creative interaction (not a task robot): the robot and child build something together through turn-taking.
- Physical, sensory-friendly medium: brush + ink drawing is softer and more expressive than digital UI.
- Unpredictability by design: real dice makes each round different, keeping the interaction playful and real.
- A demo that is instantly understandable at the table: “roll → robot draws → human draws → shared outcome.
  
- *Innovation in design, methodology, or application*
- We treat the robot as a social interaction scaffold (predictable partner), not just an automation tool.
- The interaction is designed for calmness + trust, aligned with therapeutic principles (structure, repetition, safe feedback).

### 3. Technical implementations
- *Teleoperation / Dataset capture*
	•	We calibrated and teleoperated the SO-101 arm to collect demonstrations of drawing strokes.
	•	We recorded multiple sessions of brush/ink trajectories to capture consistent line behavior.
	•	Dataset includes: camera observations + robot actions during brush drawing.

- *Training*
	•	We trained a policy from demonstrations to reproduce brush strokes reliably on paper.
	•	We iterated on data collection to improve stability and line consistency.

- *Inference*
	•	At inference time, the robot executes the learned drawing behavior in a turn-based loop after each dice roll.
	•	The system supports repeatable drawing rounds for live human interaction.
    - *<Image/video of inference eval>*

### 4. Ease of use
- *How generalizable is your implementation across tasks or environments?*
The pipeline can generalize to other turn-taking co-creation tasks (drawing shapes, patterns, collaborative art prompts).
The interaction is modular: dice rule → target pattern → robot draws.
- *Flexibility and adaptability of the solution*
Difficulty can scale: simple lines → shapes → symbols → collaborative drawings.
Can be adapted to different contexts (therapy session, classroom activity, home play).
- *Types of commands or interfaces needed to control the robot*
Minimal operator overhead: start/stop + round trigger.
Easy for non-technical facilitators: the “game rules” are the interface.

## Additional Links
*For example, you can provide links to:*

- *Link to a video of your robot performing the task*
- *URL of your dataset in Hugging Face*
- *URL of your model in Hugging Face*
- *Link to a blog post describing your work*

## Code submission

This is the directory tree of this repo, you need to fill in the `mission` directory with your submission details.

```terminal
AMD_Robotics_Hackathon_2025_ProjectTemplate-main/
├── README.md
└── mission
    ├── code
    │   └── <code and script>
    └── wandb
        └── <latest run directory copied from wandb of your training job>
```


The `latest-run` is generated by wandb for your training job. Please copy it into the wandb sub directory of you Hackathon Repo.

The whole dir of `latest-run` will look like below:

```terminal
$ tree outputs/train/smolvla_so101_2cube_30k_steps/wandb/
outputs/train/smolvla_so101_2cube_30k_steps/wandb/
├── debug-internal.log -> run-20251029_063411-tz1cpo59/logs/debug-internal.log
├── debug.log -> run-20251029_063411-tz1cpo59/logs/debug.log
├── latest-run -> run-20251029_063411-tz1cpo59
└── run-20251029_063411-tz1cpo59
    ├── files
    │   ├── config.yaml
    │   ├── output.log
    │   ├── requirements.txt
    │   ├── wandb-metadata.json
    │   └── wandb-summary.json
    ├── logs
    │   ├── debug-core.log -> /dataset/.cache/wandb/logs/core-debug-20251029_063411.log
    │   ├── debug-internal.log
    │   └── debug.log
    ├── run-tz1cpo59.wandb
    └── tmp
        └── code
```

**NOTES**

1. The `latest-run` is the soft link, please make sure to copy the real target directory it linked with all sub dirs and files.
2. Only provide (upload) the wandb of your last success pre-trained model for the Mission.
